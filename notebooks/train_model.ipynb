{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training baseline model notebook\n",
    "\n",
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ml_base.model import BaselineBirdClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = os.path.realpath('../data/train_data_s3/')\n",
    "MODEL_SAVE_PATH = os.path.realpath('../data/models')\n",
    "VAL_FRAC = 0.1\n",
    "BATCH_SIZE = 16\n",
    "SAMPLE_LEN_SEC = 10\n",
    "SAMPLE_RATE = 32000\n",
    "EPOCHS_COUNT = 2\n",
    "EVAL_EVERY_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for all downloaded audio files\n",
    "\n",
    "Use `download_data_s3.py` to download all the \"checked\" data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob(os.path.join(TRAIN_DATA_PATH, '**/*.ogg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1021"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob(os.path.join(TRAIN_DATA_PATH, '*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame({'file_path': all_files})\n",
    "all_df['class'] = all_df['file_path'].apply(lambda filepath: os.path.basename(os.path.dirname(filepath)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting class to class id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS2ID = {classname: i for i, classname in enumerate(all_df['class'].unique())}\n",
    "ID2CLASS = {i: classname for classname, i in CLASS2ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['class_id'] = all_df['class'].apply(CLASS2ID.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>class</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>asbfly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>ashdro1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>ashdro1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>ashpri1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>ashpri1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>zitcis1</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>zitcis1</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>zitcis1</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>zitcis1</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...</td>\n",
       "      <td>zitcis1</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1021 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_path    class  class_id\n",
       "0     E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...   asbfly         0\n",
       "1     E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  ashdro1         1\n",
       "2     E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  ashdro1         1\n",
       "3     E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  ashpri1         2\n",
       "4     E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  ashpri1         2\n",
       "...                                                 ...      ...       ...\n",
       "1016  E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  zitcis1       148\n",
       "1017  E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  zitcis1       148\n",
       "1018  E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  zitcis1       148\n",
       "1019  E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  zitcis1       148\n",
       "1020  E:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\t...  zitcis1       148\n",
       "\n",
       "[1021 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 102)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = all_df.sample(int(VAL_FRAC * len(all_df)))\n",
    "train_df = all_df.loc[~all_df.index.isin(val_df.index)]\n",
    "len(train_df), len(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, paths, labels=None, sample_len=SAMPLE_LEN_SEC, sr=SAMPLE_RATE):\n",
    "        assert labels is None or len(paths) == len(labels), \"Data and targets should be of the same samples count\"\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.sample_len = sample_len\n",
    "        self.sr = sr\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        audio, sr = librosa.load(self.paths[i], sr=self.sr)\n",
    "\n",
    "        if self.sample_len is not None:\n",
    "            desired_len = self.sample_len * sr\n",
    "            if len(audio) >desired_len:\n",
    "                audio = audio[:desired_len]\n",
    "            else:\n",
    "                audio =  np.pad(audio, (0, desired_len - len(audio)))\n",
    "\n",
    "        if self.labels is not None:\n",
    "            return audio, self.labels[i]\n",
    "        else:\n",
    "            return audio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = AudioDataset(train_df['file_path'].tolist(), train_df['class_id'].tolist())\n",
    "val_ds = AudioDataset(val_df['file_path'].tolist(), val_df['class_id'].tolist(), sample_len=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.1765696e-07, -3.1738637e-05, -4.2713637e-06, ...,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
       " 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Training preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT kernels created, time used = 0.0278 seconds\n"
     ]
    }
   ],
   "source": [
    "model = BaselineBirdClassifier(len(CLASS2ID), sr=SAMPLE_RATE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10. Loss: 4.997318. Val loss: 5.002830.\n",
      "Saving the model\n",
      "Batch 20. Loss: 5.001742. Val loss: 5.002751.\n",
      "Saving the model\n",
      "Batch 30. Loss: 5.001061. Val loss: 5.002618.\n",
      "Saving the model\n",
      "Batch 40. Loss: 5.004285. Val loss: 5.002512.\n",
      "Saving the model\n",
      "Batch 50. Loss: 5.001182. Val loss: 5.002398.\n",
      "Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [02:06<02:06, 126.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60. Loss: 1.001172. Val loss: 5.002195.\n",
      "Saving the model\n",
      "Batch 70. Loss: 5.000711. Val loss: 5.001924.\n",
      "Saving the model\n",
      "Batch 80. Loss: 4.998894. Val loss: 5.001608.\n",
      "Saving the model\n",
      "Batch 90. Loss: 4.998351. Val loss: 5.001476.\n",
      "Saving the model\n",
      "Batch 100. Loss: 5.001676. Val loss: 5.001209.\n",
      "Saving the model\n",
      "Batch 110. Loss: 4.998280. Val loss: 5.000982.\n",
      "Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [03:43<00:00, 111.62s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_num = 0\n",
    "\n",
    "min_eval_loss = np.inf\n",
    "for epoch in tqdm(range(EPOCHS_COUNT), desc='Epoch'):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    for audios, labels in train_loader:\n",
    "        audios = audios.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(audios)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_num % EVAL_EVERY_EPOCHS == EVAL_EVERY_EPOCHS - 1:\n",
    "            last_loss = running_loss / EVAL_EVERY_EPOCHS\n",
    "            print(f'Batch {batch_num + 1}. Loss: {last_loss:.6f}.', end=' ')\n",
    "            running_loss = 0.\n",
    "\n",
    "            model.eval()\n",
    "            eval_running_loss = 0.\n",
    "            with torch.no_grad():\n",
    "                for audios, labels in val_loader:\n",
    "                    audios = audios.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(audios)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "\n",
    "                    eval_running_loss += loss.item()\n",
    "            \n",
    "            print(f'Val loss: {eval_running_loss/len(val_ds):.6f}.')\n",
    "\n",
    "            if eval_running_loss < min_eval_loss:\n",
    "                min_eval_loss = eval_running_loss\n",
    "                print(\"Saving the model\")\n",
    "\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_SAVE_PATH, f'baseline-{len(CLASS2ID)}.pt'))\n",
    "\n",
    "            model.train()\n",
    "        batch_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX export\n",
    "\n",
    "Loading the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineBirdClassifier(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): STFT(n_fft=1024, Fourier Kernel size=(513, 1, 1024), iSTFT=False, trainable=False)\n",
       "    (1): MelScale()\n",
       "    (2): AmplitudeToDB()\n",
       "  )\n",
       "  (backbone): LSTM(64, 32, num_layers=3, batch_first=True, dropout=0.05, bidirectional=True)\n",
       "  (head): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=16, out_features=149, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, f'baseline-{len(CLASS2ID)}.pt'), map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And exporting it to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\.venv\\Lib\\site-packages\\nnAudio\\features\\stft.py:283: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.num_samples < self.pad_amount:\n",
      "e:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\notebooks\\..\\ml_base\\model.py:28: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if return_spec:\n",
      "e:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "e:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\.venv\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "e:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "e:\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\.venv\\Lib\\site-packages\\torch\\onnx\\utils.py:1208: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "torch_input = torch.randn(8, SAMPLE_RATE*SAMPLE_LEN_SEC)\n",
    "torch.onnx.export(model.cpu(),\n",
    "                  torch_input,\n",
    "                 os.path.join(MODEL_SAVE_PATH, f'baseline-{len(CLASS2ID)}.onnx'),\n",
    "                 export_params=True,\n",
    "                 do_constant_folding=True,\n",
    "                 input_names = ['input'],\n",
    "                 output_names = ['output'],\n",
    "                 dynamic_axes={'input' : {0: 'batch_size', 1: 'sample_length'},\n",
    "                               'output' : {0: 'batch_size'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that everything is OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(os.path.join(MODEL_SAVE_PATH, f'baseline-{len(CLASS2ID)}.onnx'))\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(os.path.join(MODEL_SAVE_PATH, f'baseline-{len(CLASS2ID)}.onnx'))\n",
    "outputs = ort_sess.run(None, {'input': np.random.randn(1, 128302).astype(np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5311749 , 0.5527598 , 0.5498919 , 0.4817451 , 0.55824643,\n",
       "        0.4672618 , 0.53212756, 0.48078275, 0.56325716, 0.49729684],\n",
       "       dtype=float32),\n",
       " (1, 149))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0][:10], outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
