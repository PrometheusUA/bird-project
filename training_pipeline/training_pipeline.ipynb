{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Output,\n",
    "                        Model,\n",
    "                        Metrics,\n",
    "                        Markdown,\n",
    "                        HTML,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import google.auth\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=E:\\\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\bird-project-mlops-vertex-ab4a1e84f536.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS=E:\\\\_UNIVER\\UCU\\2 sem\\MLOps\\bird-project\\data\\bird-project-mlops-vertex-ab4a1e84f536.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"bird-project-mlops-vertex\"\n",
    "REGION = 'us-central1'\n",
    "\n",
    "# BUCKET_NAME=\"gs://bird-project-mlops-vertex-bucket\"\n",
    "DATA_BUCKET=\"bird-project-mlops-vertex-data\"\n",
    "BUCKET_NAME=\"gs://bird-project-mlops-vertex-pipebucket\"\n",
    "\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/training_pipe/\"\n",
    "\n",
    "IMAGE_NAME = \"training\"\n",
    "SERVING_IMAGE_NAME = \"serving\"\n",
    "BASE_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/bird-containers/{IMAGE_NAME}\"\n",
    "SERVING_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/bird-containers/{SERVING_IMAGE_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=['google-cloud-storage==2.17.0']\n",
    ")\n",
    "def obtain_data(\n",
    "    dataset_bucket: str,\n",
    "    samples_download: int,\n",
    "    dataset_full: Output[Dataset],\n",
    "    # new_samples_count: Output[Metrics],\n",
    "    min_samples_grow: int = 1,\n",
    ") -> NamedTuple(\"Outputs\", [(\"new_samples_count\", int)]):\n",
    "    import os\n",
    "    import logging\n",
    "    from google.cloud.storage import Client, transfer_manager\n",
    "    import pandas as pd\n",
    "    from time import sleep\n",
    "    from collections import namedtuple\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    def download_bucket_with_transfer_manager(\n",
    "        bucket_name, destination_directory=\"\", workers=8, max_results=1000\n",
    "    ):\n",
    "        \"\"\"Download all of the blobs in a bucket, concurrently in a process pool.\n",
    "\n",
    "        The filename of each blob once downloaded is derived from the blob name and\n",
    "        the `destination_directory `parameter. For complete control of the filename\n",
    "        of each blob, use transfer_manager.download_many() instead.\n",
    "\n",
    "        Directories will be created automatically as needed, for instance to\n",
    "        accommodate blob names that include slashes.\n",
    "        \"\"\"\n",
    "\n",
    "        # The ID of your GCS bucket\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "        # The directory on your computer to which to download all of the files. This\n",
    "        # string is prepended (with os.path.join()) to the name of each blob to form\n",
    "        # the full path. Relative paths and absolute paths are both accepted. An\n",
    "        # empty string means \"the current working directory\". Note that this\n",
    "        # parameter allows accepts directory traversal (\"../\" etc.) and is not\n",
    "        # intended for unsanitized end user input.\n",
    "        # destination_directory = \"\"\n",
    "\n",
    "        # The maximum number of processes to use for the operation. The performance\n",
    "        # impact of this value depends on the use case, but smaller files usually\n",
    "        # benefit from a higher number of processes. Each additional process occupies\n",
    "        # some CPU and memory resources until finished. Threads can be used instead\n",
    "        # of processes by passing `worker_type=transfer_manager.THREAD`.\n",
    "        # workers=8\n",
    "\n",
    "        # The maximum number of results to fetch from bucket.list_blobs(). This\n",
    "        # sample code fetches all of the blobs up to max_results and queues them all\n",
    "        # for download at once. Though they will still be executed in batches up to\n",
    "        # the processes limit, queueing them all at once can be taxing on system\n",
    "        # memory if buckets are very large. Adjust max_results as needed for your\n",
    "        # system environment, or set it to None if you are sure the bucket is not\n",
    "        # too large to hold in memory easily.\n",
    "        # max_results=1000\n",
    "        logging.info(\"Libs imported\")\n",
    "\n",
    "        storage_client = Client()\n",
    "\n",
    "        logging.info(\"Client created\")\n",
    "\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        logging.info(\"Bucket created\")\n",
    "\n",
    "        blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n",
    "\n",
    "        logging.info(\"Blobs listed\")\n",
    "\n",
    "        transfer_manager.download_many_to_path(\n",
    "            bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n",
    "        )\n",
    "\n",
    "        logging.info(\"Blobs downloaded\")\n",
    "\n",
    "    download_bucket_with_transfer_manager(dataset_bucket, dataset_full.path, max_results = samples_download)\n",
    "\n",
    "    dataset_df_path = os.path.join(dataset_full.path, 'dataset.csv')\n",
    "    df = pd.read_csv(dataset_df_path)\n",
    "\n",
    "    if 'trained_on' in df.columns:\n",
    "        new_samples_count_val = len(df) - len(df.loc[df['trained_on']])\n",
    "    else:\n",
    "        new_samples_count_val = len(df)\n",
    "\n",
    "    if new_samples_count_val > min_samples_grow:\n",
    "        df['trained_on'] = True\n",
    "\n",
    "        df.to_csv(dataset_df_path, index=False)\n",
    "\n",
    "        storage_client = Client()\n",
    "        bucket = storage_client.bucket(dataset_bucket)\n",
    "\n",
    "        bucket.delete_blob('dataset.csv')\n",
    "\n",
    "        sleep(10)\n",
    "\n",
    "        blob = bucket.blob('dataset.csv')\n",
    "        blob.upload_from_filename(dataset_df_path, if_generation_match=0)\n",
    "    \n",
    "    outputs = namedtuple(\"Outputs\", [\"new_samples_count\"])\n",
    "    return outputs(new_samples_count_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def preprocess_data(\n",
    "    dataset_full: Input[Dataset],\n",
    "    CLASS2ID_data: Output[Dataset],\n",
    "):\n",
    "    import os\n",
    "    import json\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "   \n",
    "    dataset_df_path = os.path.join(dataset_full.path, 'dataset.csv')\n",
    "    df = pd.read_csv(dataset_df_path)\n",
    "\n",
    "    logging.info(\"Dataframe loaded\")\n",
    "\n",
    "    df['file_path'] = df['path'].apply(lambda x: f\"{dataset_full.path}/{x.split('/', 3)[-1]}\")\n",
    "    \n",
    "    df = df.loc[df['file_path'].apply(os.path.exists)]\n",
    "\n",
    "    CLASS2ID = {classname: i for i, classname in enumerate(df['label'].unique())}\n",
    "    class2id_json = json.dumps(CLASS2ID, indent=4)\n",
    "    with open(CLASS2ID_data.path, 'w') as class2id_file:\n",
    "        class2id_file.write(class2id_json)\n",
    "\n",
    "    df['label_id'] = df['label'].apply(CLASS2ID.get)\n",
    "\n",
    "    df.to_csv(dataset_df_path, index=False)\n",
    "\n",
    "    logging.info(\"Dataframe saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def train_test_split(dataset_full: Input[Dataset],\n",
    "                     dataset_train: Output[Dataset],\n",
    "                     dataset_test: Output[Dataset],\n",
    "                     test_size: float = 0.05):\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    dataset_df_path = os.path.join(dataset_full.path, 'dataset.csv')\n",
    "    df = pd.read_csv(dataset_df_path)\n",
    "\n",
    "    df_test = df.sample(int(test_size * len(df)))\n",
    "    df_train = df.loc[~df.index.isin(df_test.index)]\n",
    "\n",
    "    logging.info(\"Dataframe splitted\")\n",
    "\n",
    "    df_train.to_csv(dataset_train.path, index=False)\n",
    "    df_test.to_csv(dataset_test.path, index=False)\n",
    "    logging.info(\"Dataframes splitted saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def train_model(dataset_train: Input[Dataset],\n",
    "                CLASS2ID_data: Input[Dataset],\n",
    "                model_out: Output[Model],\n",
    "                batch_size: int = 16,\n",
    "                epochs_count: int = 10):\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from torch.utils.data import DataLoader\n",
    "    from tqdm import tqdm\n",
    "    import logging\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    from src.dataset import AudioDataset, SAMPLE_LEN_SEC, SAMPLE_RATE\n",
    "    from src.model import BaselineBirdClassifier\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    with open(CLASS2ID_data.path, 'r') as class2id_file:\n",
    "        CLASS2ID = json.load(class2id_file)\n",
    "\n",
    "    train_df = pd.read_csv(dataset_train.path)\n",
    "\n",
    "    train_ds = AudioDataset(train_df['file_path'].tolist(), train_df['label_id'].tolist())\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    logging.info(\"Dataset and dataloader for train created\")\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = BaselineBirdClassifier(len(CLASS2ID), sr=SAMPLE_RATE).to(device)\n",
    "\n",
    "    logging.info(\"Model created\")\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RAdam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    batch_num = 0\n",
    "\n",
    "    for epoch in tqdm(range(epochs_count), desc='Epoch'):\n",
    "        for audios, labels in train_loader:\n",
    "            audios = audios.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(audios)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()            \n",
    "            batch_num += 1\n",
    "    \n",
    "    logging.info(\"Model trained\")\n",
    "    \n",
    "    torch.save(model.state_dict(), model_out.path)\n",
    "\n",
    "    logging.info(\"Model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def eval_model(dataset_test: Input[Dataset],\n",
    "                CLASS2ID_data: Input[Dataset],\n",
    "                model_out: Input[Model],\n",
    "                metrics: Output[Metrics],\n",
    "                main_metric: str) -> NamedTuple(\"Outputs\", [(\"main_metric_val\", float)]):\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from torch.utils.data import DataLoader\n",
    "    import logging\n",
    "    import os\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    from src.dataset import AudioDataset, SAMPLE_RATE, obtain_metrics\n",
    "    from src.model import BaselineBirdClassifier\n",
    "\n",
    "    with open(CLASS2ID_data.path, 'r') as class2id_file:\n",
    "        CLASS2ID = json.load(class2id_file)\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    test_df = pd.read_csv(dataset_test.path)\n",
    "\n",
    "    test_ds = AudioDataset(test_df['file_path'].tolist(), test_df['label_id'].tolist())\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    logging.info(\"Test dataset and dataloader created\")\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    model = BaselineBirdClassifier(len(CLASS2ID), sr=SAMPLE_RATE)\n",
    "    model.load_state_dict(torch.load(model_out.path, map_location=device))\n",
    "\n",
    "    logging.info(\"Model loaded\")\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    eval_running_loss = 0.\n",
    "    outputs_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for audios, labels in test_loader:\n",
    "            audios = audios.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(audios)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            eval_running_loss += loss.item()\n",
    "            outputs_list.append(outputs.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    \n",
    "    logging.info(\"Test dataset forward pass finished\")\n",
    "\n",
    "    eval_running_loss = eval_running_loss/len(test_loader.dataset)\n",
    "\n",
    "    outputs = np.concatenate(outputs_list, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "\n",
    "    test_metrics = obtain_metrics(labels, outputs)\n",
    "\n",
    "    logging.info(\"Metrics calculated\")\n",
    "\n",
    "    for metric_name, val in test_metrics.items():\n",
    "        metrics.log_metric(metric_name, float(val))\n",
    "    \n",
    "    logging.info(\"Test metrics logged\")\n",
    "\n",
    "    outputs = namedtuple(\"Outputs\", [\"main_metric_val\"])\n",
    "    return outputs(test_metrics[main_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=['google-cloud-storage==2.17.0', 'google-cloud-aiplatform==1.59.0', 'onnx==1.16.1','onnxscript==0.1.0.dev20240528']\n",
    ")\n",
    "def deploy_model(\n",
    "        serving_container_image_uri: str,\n",
    "        display_name: str,\n",
    "        model_endpoint: str,\n",
    "        gcp_project: str,\n",
    "        gcp_region: str,\n",
    "        model: Input[Model],\n",
    "        CLASS2ID_data: Input[Dataset],\n",
    "        model_onnx: Output[Model],\n",
    "        vertex_model: Output[Model],\n",
    "        vertex_endpoint: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import torch\n",
    "    import logging\n",
    "\n",
    "    from src.dataset import SAMPLE_RATE, SAMPLE_LEN_SEC\n",
    "    from src.model import BaselineBirdClassifier\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    with open(CLASS2ID_data.path, 'r') as class2id_file:\n",
    "        CLASS2ID = json.load(class2id_file)\n",
    "\n",
    "    # Checks existing Vertex AI Enpoint or creates Endpoint if it is not exist.\n",
    "    def create_endpoint ():\n",
    "        endpoints = vertex_ai.Endpoint.list(\n",
    "            filter=f'display_name=\"{model_endpoint}\"',\n",
    "            order_by='create_time desc',\n",
    "            project=gcp_project,\n",
    "            location=gcp_region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0] # most recently created\n",
    "        else:\n",
    "            endpoint = vertex_ai.Endpoint.create(\n",
    "                display_name=model_endpoint,\n",
    "                project=gcp_project,\n",
    "                location=gcp_region\n",
    "        )\n",
    "        return endpoint\n",
    "\n",
    "    endpoint = create_endpoint()\n",
    "    \n",
    "    logging.info(\"Endpoint created\")\n",
    "\n",
    "    # Uploads trained model to Vertex AI Model Registry or creates new model version into existing uploaded one.\n",
    "    def upload_model():\n",
    "        listed_model = vertex_ai.Model.list(\n",
    "            filter=f'display_name=\"{display_name}\"',\n",
    "            project=gcp_project,\n",
    "            location=gcp_region,\n",
    "        )\n",
    "        if len(listed_model) > 0:\n",
    "            model_version = listed_model[0] # most recently created\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    parent_model=model_version.resource_name,\n",
    "                    artifact_uri=str(Path(model_onnx.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        else:\n",
    "            model_upload = vertex_ai.Model.upload(\n",
    "                    display_name=display_name,\n",
    "                    artifact_uri=str(Path(model_onnx.path).parent),\n",
    "                    serving_container_image_uri=serving_container_image_uri,\n",
    "                    location=gcp_region,\n",
    "                    serving_container_predict_route=\"/predict\",\n",
    "                    serving_container_health_route=\"/health\"\n",
    "            )\n",
    "        return model_upload\n",
    "    \n",
    "    model_pt = BaselineBirdClassifier(len(CLASS2ID), sr=SAMPLE_RATE)\n",
    "    model_pt.load_state_dict(torch.load(model.path, map_location='cpu'))\n",
    "    model_pt.eval()\n",
    "\n",
    "    logging.info(\"Torch model downloaded\")\n",
    "\n",
    "    torch_input = torch.randn(8, SAMPLE_RATE*SAMPLE_LEN_SEC)\n",
    "    torch.onnx.export(model_pt.cpu(),\n",
    "                    torch_input,\n",
    "                    model_onnx.path,\n",
    "                    export_params=True,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names = ['input'],\n",
    "                    output_names = ['output'],\n",
    "                    dynamic_axes={'input' : {0: 'batch_size', 1: 'sample_length'},\n",
    "                                'output' : {0: 'batch_size'}}\n",
    "    )\n",
    "\n",
    "    logging.info(\"ONNX model created\")\n",
    "\n",
    "    uploaded_model = upload_model()\n",
    "    \n",
    "    logging.info(\"Model uploaded\")\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = uploaded_model.resource_name\n",
    "\n",
    "    # Deploys trained model to Vertex AI Endpoint\n",
    "    model_deploy = uploaded_model.deploy(\n",
    "        machine_type='e2-standard-4',\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=display_name,\n",
    "    )\n",
    "\n",
    "    logging.info(\"Model deployed\")\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = f'birds' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_21512\\1458993980.py:20: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(data_op.outputs[\"new_samples_count\"] > min_samples_grow, \"enough-new-data\"):\n",
      "C:\\Users\\Andrii\\AppData\\Local\\Temp\\ipykernel_21512\\1458993980.py:25: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(model_evaluation_op.outputs['main_metric_val'] > main_metric_thresh, 'save-model-choice'):\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=\"bird-pipeline\"   \n",
    ")\n",
    "def pipeline(\n",
    "    data_filepath: str = DATA_BUCKET,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION, \n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    serving_container_image_uri: str = SERVING_IMAGE,\n",
    "    test_size: float = 0.05,\n",
    "    batch_size: int = 16,\n",
    "    epochs_count: int = 1,\n",
    "    samples_download: int = 40,\n",
    "    min_samples_grow: int = -1,\n",
    "    main_metric: str = 'macro_f1',\n",
    "    main_metric_thresh: float = 0.75,\n",
    "):\n",
    "    data_op = obtain_data(dataset_bucket=data_filepath, samples_download=samples_download, min_samples_grow=min_samples_grow)\n",
    "    with dsl.Condition(data_op.outputs[\"new_samples_count\"] > min_samples_grow, \"enough-new-data\"):\n",
    "        data_preprocess_op = preprocess_data(dataset_full=data_op.outputs[\"dataset_full\"])\n",
    "        train_test_split_op = train_test_split(dataset_full=data_op.outputs[\"dataset_full\"], test_size=test_size).after(data_preprocess_op)\n",
    "        train_model_op = train_model(dataset_train=train_test_split_op.outputs[\"dataset_train\"], CLASS2ID_data=data_preprocess_op.outputs[\"CLASS2ID_data\"], batch_size=batch_size, epochs_count=epochs_count)\n",
    "        model_evaluation_op = eval_model(dataset_test=train_test_split_op.outputs[\"dataset_test\"], CLASS2ID_data=data_preprocess_op.outputs[\"CLASS2ID_data\"], model_out=train_model_op.outputs[\"model_out\"], main_metric=main_metric)\n",
    "        with dsl.Condition(model_evaluation_op.outputs['main_metric_val'] > main_metric_thresh, 'save-model-choice'):\n",
    "            deploy_op = deploy_model(model = train_model_op.outputs['model_out'],\n",
    "                gcp_project = project,\n",
    "                gcp_region = region, \n",
    "                serving_container_image_uri = serving_container_image_uri,\n",
    "                display_name = display_name,\n",
    "                model_endpoint = f\"{display_name}_endpoint\",\n",
    "                CLASS2ID_data=data_preprocess_op.outputs[\"CLASS2ID_data\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "    package_path='bird-pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials, project = google.auth.default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project = PROJECT_ID,\n",
    "                location = REGION,\n",
    "                credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline-birds\",\n",
    "    template_path=\"bird-pipeline.json\",\n",
    "    enable_caching=True,\n",
    "    job_id=f\"bird-pipeline-{TIMESTAMP}\",\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'samples_download': 100,\n",
    "        'main_metric': 'macro_precision',\n",
    "        'epochs_count': 2,\n",
    "        'main_metric_thresh': 0.5\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/bird-pipeline-20240720144940?project=64206774286\n",
      "PipelineJob projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940 current state:\n",
      "3\n",
      "PipelineJob projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940 current state:\n",
      "3\n",
      "PipelineJob projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940 current state:\n",
      "3\n",
      "PipelineJob projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940 current state:\n",
      "3\n",
      "PipelineJob projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940 current state:\n",
      "3\n",
      "PipelineJob projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940 current state:\n",
      "3\n",
      "PipelineJob projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940 current state:\n",
      "3\n",
      "PipelineJob run completed. Resource name: projects/64206774286/locations/us-central1/pipelineJobs/bird-pipeline-20240720144940\n"
     ]
    }
   ],
   "source": [
    "start_pipeline.run(service_account=\"bird-google-storage-account@bird-project-mlops-vertex.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_pipeline.create_schedule(\n",
    "#   display_name=\"bird-schedule\",\n",
    "#   cron=\"TZ=0 9 * * *\",\n",
    "#   max_concurrent_run_count=2,\n",
    "#   max_run_count=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
